{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 2: Prompt Engineering & RAG\n",
    "\n",
    "This notebook covers:\n",
    "- Advanced Prompt Engineering Techniques\n",
    "- Retrieval Augmented Generation (RAG)\n",
    "- Vector Databases and Embeddings\n",
    "- Building a Complete RAG System\n",
    "\n",
    "**Prerequisites**: Install required packages:\n",
    "```bash\n",
    "pip install openai anthropic langchain langchain-openai langchain-anthropic langchain-community chromadb faiss-cpu sentence-transformers pypdf\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Advanced Prompt Engineering\n",
    "\n",
    "### 1.1 Few-Shot Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "def zero_shot_classification(text):\n",
    "    \"\"\"Zero-shot sentiment classification.\"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"Classify sentiment as positive or negative.\"},\n",
    "            {\"role\": \"user\", \"content\": text}\n",
    "        ],\n",
    "        temperature=0\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "def few_shot_classification(text):\n",
    "    \"\"\"Few-shot sentiment classification with examples.\"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"Classify sentiment as positive or negative.\"},\n",
    "            {\"role\": \"user\", \"content\": \"I love this product!\"},\n",
    "            {\"role\": \"assistant\", \"content\": \"positive\"},\n",
    "            {\"role\": \"user\", \"content\": \"This is terrible.\"},\n",
    "            {\"role\": \"assistant\", \"content\": \"negative\"},\n",
    "            {\"role\": \"user\", \"content\": \"Best purchase ever!\"},\n",
    "            {\"role\": \"assistant\", \"content\": \"positive\"},\n",
    "            {\"role\": \"user\", \"content\": text}\n",
    "        ],\n",
    "        temperature=0\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "test_text = \"Not impressed with the quality.\"\n",
    "print(f\"Text: {test_text}\")\n",
    "print(f\"Zero-shot: {zero_shot_classification(test_text)}\")\n",
    "print(f\"Few-shot: {few_shot_classification(test_text)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Chain-of-Thought Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_with_cot(problem):\n",
    "    \"\"\"Use Chain-of-Thought prompting for reasoning.\"\"\"\n",
    "    prompt = f\"\"\"\n",
    "Solve this problem step by step:\n",
    "\n",
    "{problem}\n",
    "\n",
    "Show your reasoning:\n",
    "1. Break down the problem\n",
    "2. Show intermediate steps\n",
    "3. Provide the final answer\n",
    "\"\"\"\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "problem = \"\"\"A store has 100 apples. They sell 40% on Monday and 30% of the \n",
    "remaining apples on Tuesday. How many apples are left?\"\"\"\n",
    "\n",
    "print(solve_with_cot(problem))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Structured Output with Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def extract_structured_info(text):\n",
    "    \"\"\"Extract structured information from text.\"\"\"\n",
    "    prompt = f\"\"\"\n",
    "Extract the following information from the text and return as JSON:\n",
    "- product_name\n",
    "- price\n",
    "- rating (out of 5)\n",
    "- pros (list)\n",
    "- cons (list)\n",
    "\n",
    "Text: {text}\n",
    "\n",
    "Return only valid JSON, no additional text.\n",
    "\"\"\"\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0\n",
    "    )\n",
    "    \n",
    "    return json.loads(response.choices[0].message.content)\n",
    "\n",
    "review = \"\"\"\n",
    "The XPhone Pro costs $999 and it's incredible! The camera quality is outstanding \n",
    "and battery life lasts all day. However, it's quite expensive and a bit heavy.\n",
    "I'd rate it 4 out of 5 stars.\n",
    "\"\"\"\n",
    "\n",
    "result = extract_structured_info(review)\n",
    "print(json.dumps(result, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Role-Based Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expert_advice(question, expert_role):\n",
    "    \"\"\"Get advice from an expert persona.\"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": f\"You are an expert {expert_role}. Provide detailed, professional advice.\"},\n",
    "            {\"role\": \"user\", \"content\": question}\n",
    "        ],\n",
    "        temperature=0.7\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "question = \"How can I improve the performance of my NLP model?\"\n",
    "\n",
    "print(\"Machine Learning Engineer perspective:\")\n",
    "print(expert_advice(question, \"Machine Learning Engineer\"))\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "print(\"Data Scientist perspective:\")\n",
    "print(expert_advice(question, \"Data Scientist\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Introduction to RAG (Retrieval Augmented Generation)\n",
    "\n",
    "RAG combines retrieval systems with language models to provide accurate, contextual responses based on specific documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Understanding Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import numpy as np\n",
    "\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "def get_embedding(text, model=\"text-embedding-3-small\"):\n",
    "    \"\"\"Get embedding for text using OpenAI.\"\"\"\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    response = client.embeddings.create(input=[text], model=model)\n",
    "    return response.data[0].embedding\n",
    "\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    \"\"\"Calculate cosine similarity between two vectors.\"\"\"\n",
    "    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
    "\n",
    "# Example texts\n",
    "texts = [\n",
    "    \"The cat sat on the mat.\",\n",
    "    \"A feline rested on the rug.\",\n",
    "    \"The stock market crashed today.\"\n",
    "]\n",
    "\n",
    "# Get embeddings\n",
    "embeddings = [get_embedding(text) for text in texts]\n",
    "\n",
    "print(f\"Embedding dimension: {len(embeddings[0])}\\n\")\n",
    "\n",
    "# Compare similarities\n",
    "print(\"Cosine Similarities:\")\n",
    "print(f\"Text 1 vs Text 2 (similar meaning): {cosine_similarity(embeddings[0], embeddings[1]):.4f}\")\n",
    "print(f\"Text 1 vs Text 3 (different meaning): {cosine_similarity(embeddings[0], embeddings[2]):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Building a Simple RAG System with LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# Sample documents\n",
    "documents = [\n",
    "    \"\"\"Python is a high-level programming language known for its simplicity and readability. \n",
    "    It was created by Guido van Rossum and first released in 1991. Python supports multiple \n",
    "    programming paradigms including procedural, object-oriented, and functional programming.\"\"\",\n",
    "    \n",
    "    \"\"\"Machine learning is a subset of artificial intelligence that enables systems to learn \n",
    "    and improve from experience without being explicitly programmed. It focuses on developing \n",
    "    algorithms that can access data and use it to learn for themselves.\"\"\",\n",
    "    \n",
    "    \"\"\"Natural Language Processing (NLP) is a branch of AI that helps computers understand, \n",
    "    interpret, and manipulate human language. NLP combines computational linguistics with \n",
    "    statistical machine learning and deep learning models.\"\"\",\n",
    "    \n",
    "    \"\"\"Deep learning is a subset of machine learning based on artificial neural networks. \n",
    "    It uses multiple layers to progressively extract higher-level features from raw input. \n",
    "    Common applications include computer vision, speech recognition, and NLP.\"\"\"\n",
    "]\n",
    "\n",
    "# Convert to Document objects\n",
    "docs = [Document(page_content=doc) for doc in documents]\n",
    "\n",
    "# Split documents into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=200,\n",
    "    chunk_overlap=20\n",
    ")\n",
    "splits = text_splitter.split_documents(docs)\n",
    "\n",
    "print(f\"Number of document chunks: {len(splits)}\")\n",
    "print(f\"\\nFirst chunk:\\n{splits[0].page_content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create embeddings and vector store\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=embeddings)\n",
    "\n",
    "# Create retriever\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 2})\n",
    "\n",
    "# Test retrieval\n",
    "query = \"What is NLP?\"\n",
    "retrieved_docs = retriever.invoke(query)\n",
    "\n",
    "print(f\"Query: {query}\\n\")\n",
    "print(\"Retrieved documents:\")\n",
    "for i, doc in enumerate(retrieved_docs, 1):\n",
    "    print(f\"\\n{i}. {doc.page_content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create RAG chain\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True\n",
    ")\n",
    "\n",
    "# Ask questions\n",
    "questions = [\n",
    "    \"What is NLP and what does it combine?\",\n",
    "    \"Who created Python?\",\n",
    "    \"What are applications of deep learning?\"\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "    result = qa_chain.invoke({\"query\": question})\n",
    "    print(f\"\\nQuestion: {question}\")\n",
    "    print(f\"Answer: {result['result']}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 RAG with Anthropic Claude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_anthropic import ChatAnthropic\n",
    "\n",
    "# Create RAG chain with Claude\n",
    "claude_llm = ChatAnthropic(model=\"claude-3-5-sonnet-20241022\", temperature=0)\n",
    "\n",
    "qa_chain_claude = RetrievalQA.from_chain_type(\n",
    "    llm=claude_llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True\n",
    ")\n",
    "\n",
    "question = \"Compare machine learning and deep learning.\"\n",
    "result = qa_chain_claude.invoke({\"query\": question})\n",
    "\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"\\nAnswer: {result['result']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 RAG with Different Vector Stores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Using FAISS (Facebook AI Similarity Search)\n",
    "faiss_vectorstore = FAISS.from_documents(splits, embeddings)\n",
    "faiss_retriever = faiss_vectorstore.as_retriever(search_kwargs={\"k\": 2})\n",
    "\n",
    "# Create QA chain with FAISS\n",
    "qa_chain_faiss = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=faiss_retriever\n",
    ")\n",
    "\n",
    "question = \"What programming paradigms does Python support?\"\n",
    "result = qa_chain_faiss.invoke({\"query\": question})\n",
    "\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Answer: {result['result']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Advanced RAG: Document Loading and Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample documents\n",
    "sample_docs = {\n",
    "    \"ai_basics.txt\": \"\"\"\n",
    "Artificial Intelligence (AI) Overview\n",
    "\n",
    "Artificial Intelligence refers to the simulation of human intelligence in machines. \n",
    "It encompasses various subfields including:\n",
    "\n",
    "1. Machine Learning: Algorithms that improve through experience\n",
    "2. Deep Learning: Neural networks with multiple layers\n",
    "3. Natural Language Processing: Understanding and generating human language\n",
    "4. Computer Vision: Interpreting and analyzing visual information\n",
    "5. Robotics: Intelligent physical systems\n",
    "\n",
    "Applications of AI include autonomous vehicles, medical diagnosis, \n",
    "recommendation systems, and virtual assistants.\n",
    "\"\"\",\n",
    "    \"ml_techniques.txt\": \"\"\"\n",
    "Machine Learning Techniques\n",
    "\n",
    "Supervised Learning:\n",
    "- Classification: Categorizing data into classes\n",
    "- Regression: Predicting continuous values\n",
    "\n",
    "Unsupervised Learning:\n",
    "- Clustering: Grouping similar data points\n",
    "- Dimensionality Reduction: Reducing feature space\n",
    "\n",
    "Reinforcement Learning:\n",
    "- Agent learns through interaction with environment\n",
    "- Receives rewards or penalties for actions\n",
    "\n",
    "Common algorithms include decision trees, random forests, \n",
    "support vector machines, and neural networks.\n",
    "\"\"\"\n",
    "}\n",
    "\n",
    "# Write sample files\n",
    "import os\n",
    "os.makedirs(\"sample_docs\", exist_ok=True)\n",
    "\n",
    "for filename, content in sample_docs.items():\n",
    "    with open(f\"sample_docs/{filename}\", \"w\") as f:\n",
    "        f.write(content)\n",
    "\n",
    "print(\"Sample documents created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader, TextLoader\n",
    "\n",
    "# Load documents from directory\n",
    "loader = DirectoryLoader(\"sample_docs\", glob=\"*.txt\", loader_cls=TextLoader)\n",
    "loaded_docs = loader.load()\n",
    "\n",
    "print(f\"Loaded {len(loaded_docs)} documents\")\n",
    "\n",
    "# Split and create vector store\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=300,\n",
    "    chunk_overlap=50\n",
    ")\n",
    "doc_splits = text_splitter.split_documents(loaded_docs)\n",
    "\n",
    "# Create vector store\n",
    "vectorstore_docs = FAISS.from_documents(doc_splits, embeddings)\n",
    "retriever_docs = vectorstore_docs.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "# Create QA chain\n",
    "qa_chain_docs = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever_docs,\n",
    "    return_source_documents=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query the document-based RAG system\n",
    "questions = [\n",
    "    \"What are the main subfields of AI?\",\n",
    "    \"Explain reinforcement learning.\",\n",
    "    \"What is the difference between supervised and unsupervised learning?\"\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "    result = qa_chain_docs.invoke({\"query\": question})\n",
    "    print(f\"\\nQuestion: {question}\")\n",
    "    print(f\"Answer: {result['result']}\")\n",
    "    print(f\"\\nSources: {len(result['source_documents'])} documents\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. RAG with Conversation Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "# Create memory\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True,\n",
    "    output_key=\"answer\"\n",
    ")\n",
    "\n",
    "# Create conversational RAG chain\n",
    "conversational_chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm,\n",
    "    retriever=retriever_docs,\n",
    "    memory=memory,\n",
    "    return_source_documents=True\n",
    ")\n",
    "\n",
    "# Have a conversation\n",
    "print(\"Conversational RAG Demo:\\n\")\n",
    "\n",
    "q1 = \"What is machine learning?\"\n",
    "result1 = conversational_chain.invoke({\"question\": q1})\n",
    "print(f\"Q: {q1}\")\n",
    "print(f\"A: {result1['answer']}\\n\")\n",
    "\n",
    "q2 = \"What are its main types?\"\n",
    "result2 = conversational_chain.invoke({\"question\": q2})\n",
    "print(f\"Q: {q2}\")\n",
    "print(f\"A: {result2['answer']}\\n\")\n",
    "\n",
    "q3 = \"Give me an example of the first type.\"\n",
    "result3 = conversational_chain.invoke({\"question\": q3})\n",
    "print(f\"Q: {q3}\")\n",
    "print(f\"A: {result3['answer']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluating RAG Systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_rag_response(question, answer, source_docs):\n",
    "    \"\"\"Evaluate RAG response quality.\"\"\"\n",
    "    evaluation_prompt = f\"\"\"\n",
    "Evaluate this RAG system response:\n",
    "\n",
    "Question: {question}\n",
    "Answer: {answer}\n",
    "\n",
    "Source Documents:\n",
    "{chr(10).join([f\"{i+1}. {doc.page_content[:200]}...\" for i, doc in enumerate(source_docs)])}\n",
    "\n",
    "Evaluate on:\n",
    "1. Relevance: Is the answer relevant to the question? (1-5)\n",
    "2. Accuracy: Is the answer factually correct based on sources? (1-5)\n",
    "3. Completeness: Does it fully address the question? (1-5)\n",
    "4. Groundedness: Is it based on the provided sources? (1-5)\n",
    "\n",
    "Provide scores and brief explanation.\n",
    "\"\"\"\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[{\"role\": \"user\", \"content\": evaluation_prompt}],\n",
    "        temperature=0\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Evaluate a response\n",
    "question = \"What are applications of AI?\"\n",
    "result = qa_chain_docs.invoke({\"query\": question})\n",
    "\n",
    "print(\"RAG Evaluation:\\n\")\n",
    "print(evaluate_rag_response(question, result['result'], result['source_documents']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Assignment 1: Build Your Own RAG System\n",
    "\n",
    "### Task:\n",
    "Build a RAG system for a specific domain (e.g., university policies, product manuals, research papers).\n",
    "\n",
    "### Requirements:\n",
    "1. Collect or create at least 5 documents\n",
    "2. Implement proper text splitting and chunking\n",
    "3. Use embeddings and vector store\n",
    "4. Create a question-answering interface\n",
    "5. Add conversation memory\n",
    "6. Evaluate your system with test questions\n",
    "\n",
    "### Deliverables:\n",
    "- Working code\n",
    "- Sample documents\n",
    "- Test questions and responses\n",
    "- Evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your assignment code here\n",
    "\n",
    "# 1. Document Collection\n",
    "# TODO: Load your documents\n",
    "\n",
    "# 2. Text Splitting\n",
    "# TODO: Split documents into chunks\n",
    "\n",
    "# 3. Vector Store\n",
    "# TODO: Create embeddings and vector store\n",
    "\n",
    "# 4. QA Interface\n",
    "# TODO: Create RAG chain\n",
    "\n",
    "# 5. Conversation Memory\n",
    "# TODO: Add memory to your chain\n",
    "\n",
    "# 6. Evaluation\n",
    "# TODO: Test and evaluate your system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, you learned:\n",
    "1. Advanced prompt engineering techniques (few-shot, chain-of-thought, role-based)\n",
    "2. Embeddings and semantic similarity\n",
    "3. Building RAG systems with LangChain\n",
    "4. Different vector stores (Chroma, FAISS)\n",
    "5. Document loading and processing\n",
    "6. Conversational RAG with memory\n",
    "7. Evaluating RAG system quality\n",
    "\n",
    "**Next Steps**: Complete Assignment 1 and prepare for the live session on adapter tuning!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
