{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 1: Introduction to NLP\n",
    "\n",
    "This notebook covers:\n",
    "- NLP Tasks Overview\n",
    "- Data Preprocessing\n",
    "- Tokenization\n",
    "- Prompting and Zero-shot Inference\n",
    "\n",
    "**Prerequisites**: Install required packages:\n",
    "```bash\n",
    "pip install openai anthropic transformers datasets nltk spacy\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Common NLP Tasks\n",
    "\n",
    "Natural Language Processing encompasses various tasks:\n",
    "\n",
    "1. **Text Classification**: Categorizing text into predefined classes\n",
    "2. **Named Entity Recognition (NER)**: Identifying entities like names, locations, organizations\n",
    "3. **Sentiment Analysis**: Determining the emotional tone of text\n",
    "4. **Question Answering**: Extracting answers from context\n",
    "5. **Text Summarization**: Creating concise summaries\n",
    "6. **Machine Translation**: Converting text between languages\n",
    "7. **Text Generation**: Creating coherent text based on prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preprocessing\n",
    "\n",
    "Preprocessing is essential for preparing text data for NLP models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('averaged_perceptron_tagger', quiet=True)\n",
    "nltk.download('punkt_tab', quiet=True)\n",
    "\n",
    "# Sample text\n",
    "text = \"Natural Language Processing (NLP) is AMAZING! It helps computers understand human language. #AI #ML\"\n",
    "\n",
    "print(\"Original text:\")\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Lowercasing\n",
    "text_lower = text.lower()\n",
    "print(\"\\n1. Lowercased:\")\n",
    "print(text_lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Remove URLs, mentions, and hashtags\n",
    "def clean_text(text):\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    # Remove mentions and hashtags\n",
    "    text = re.sub(r'@\\w+|#\\w+', '', text)\n",
    "    # Remove special characters and numbers\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    return text\n",
    "\n",
    "text_clean = clean_text(text_lower)\n",
    "print(\"\\n2. Cleaned text:\")\n",
    "print(text_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Tokenization\n",
    "tokens = word_tokenize(text_clean)\n",
    "print(\"\\n3. Tokens:\")\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Remove stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "print(\"\\n4. Tokens without stopwords:\")\n",
    "print(filtered_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Stemming\n",
    "stemmer = PorterStemmer()\n",
    "stemmed = [stemmer.stem(word) for word in filtered_tokens]\n",
    "print(\"\\n5. Stemmed tokens:\")\n",
    "print(stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Lemmatization (preferred over stemming)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized = [lemmatizer.lemmatize(word) for word in filtered_tokens]\n",
    "print(\"\\n6. Lemmatized tokens:\")\n",
    "print(lemmatized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Modern Tokenization with Transformers\n",
    "\n",
    "Modern NLP models use subword tokenization for better handling of rare words and morphology."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Different tokenizers for different models\n",
    "models = [\n",
    "    \"bert-base-uncased\",\n",
    "    \"gpt2\",\n",
    "    \"t5-small\",\n",
    "    \"facebook/bart-base\"\n",
    "]\n",
    "\n",
    "sample_text = \"Tokenization is fundamental for NLP!\"\n",
    "\n",
    "for model_name in models:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    tokens = tokenizer.tokenize(sample_text)\n",
    "    token_ids = tokenizer.encode(sample_text)\n",
    "    \n",
    "    print(f\"\\n{model_name}:\")\n",
    "    print(f\"  Tokens: {tokens}\")\n",
    "    print(f\"  Token IDs: {token_ids}\")\n",
    "    print(f\"  Decoded: {tokenizer.decode(token_ids)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Special Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "print(\"Special tokens:\")\n",
    "print(f\"  PAD token: {tokenizer.pad_token} (ID: {tokenizer.pad_token_id})\")\n",
    "print(f\"  CLS token: {tokenizer.cls_token} (ID: {tokenizer.cls_token_id})\")\n",
    "print(f\"  SEP token: {tokenizer.sep_token} (ID: {tokenizer.sep_token_id})\")\n",
    "print(f\"  UNK token: {tokenizer.unk_token} (ID: {tokenizer.unk_token_id})\")\n",
    "print(f\"  Vocab size: {tokenizer.vocab_size}\")\n",
    "\n",
    "# Tokenize with special tokens\n",
    "encoded = tokenizer.encode_plus(\n",
    "    \"Hello world!\",\n",
    "    add_special_tokens=True,\n",
    "    return_attention_mask=True,\n",
    "    padding='max_length',\n",
    "    max_length=10\n",
    ")\n",
    "\n",
    "print(\"\\nEncoded output:\")\n",
    "print(f\"  Input IDs: {encoded['input_ids']}\")\n",
    "print(f\"  Attention Mask: {encoded['attention_mask']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Prompting and Zero-Shot Inference\n",
    "\n",
    "Modern LLMs can perform tasks without training by using carefully crafted prompts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Zero-Shot Text Classification with OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "# Initialize OpenAI client (requires OPENAI_API_KEY environment variable)\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "def classify_sentiment_openai(text):\n",
    "    \"\"\"Classify sentiment using zero-shot prompting with OpenAI.\"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a sentiment analysis expert. Classify the sentiment as positive, negative, or neutral.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"Classify the sentiment of this text: '{text}'\\n\\nRespond with only one word: positive, negative, or neutral.\"}\n",
    "        ],\n",
    "        temperature=0\n",
    "    )\n",
    "    return response.choices[0].message.content.strip().lower()\n",
    "\n",
    "# Test examples\n",
    "examples = [\n",
    "    \"I love this product! It's amazing!\",\n",
    "    \"This is terrible. I want my money back.\",\n",
    "    \"The product arrived on time.\"\n",
    "]\n",
    "\n",
    "print(\"Sentiment Classification (OpenAI):\")\n",
    "for text in examples:\n",
    "    sentiment = classify_sentiment_openai(text)\n",
    "    print(f\"  Text: {text}\")\n",
    "    print(f\"  Sentiment: {sentiment}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Zero-Shot with Anthropic Claude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import anthropic\n",
    "\n",
    "# Initialize Anthropic client (requires ANTHROPIC_API_KEY environment variable)\n",
    "client_anthropic = anthropic.Anthropic(api_key=os.getenv(\"ANTHROPIC_API_KEY\"))\n",
    "\n",
    "def extract_entities_claude(text):\n",
    "    \"\"\"Extract named entities using Claude.\"\"\"\n",
    "    message = client_anthropic.messages.create(\n",
    "        model=\"claude-3-5-sonnet-20241022\",\n",
    "        max_tokens=1024,\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": f\"Extract all named entities (people, organizations, locations) from this text: '{text}'\\n\\nProvide the response as a JSON object with keys: people, organizations, locations.\"}\n",
    "        ]\n",
    "    )\n",
    "    return message.content[0].text\n",
    "\n",
    "text = \"Apple Inc. CEO Tim Cook announced new products in Cupertino, California. Elon Musk from Tesla also attended.\"\n",
    "\n",
    "print(\"Named Entity Recognition (Claude):\")\n",
    "print(extract_entities_claude(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Zero-Shot with Hugging Face Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Zero-shot classification pipeline\n",
    "classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
    "\n",
    "text = \"I am excited to learn about natural language processing!\"\n",
    "candidate_labels = [\"education\", \"technology\", \"sports\", \"politics\"]\n",
    "\n",
    "result = classifier(text, candidate_labels)\n",
    "\n",
    "print(\"Zero-Shot Classification (Hugging Face):\")\n",
    "print(f\"Text: {text}\\n\")\n",
    "for label, score in zip(result['labels'], result['scores']):\n",
    "    print(f\"  {label}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Multiple NLP Tasks with Zero-Shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarization\n",
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "\n",
    "article = \"\"\"\n",
    "Artificial intelligence is transforming healthcare in unprecedented ways. Machine learning algorithms \n",
    "can now detect diseases earlier and more accurately than traditional methods. Natural language processing \n",
    "helps doctors extract insights from medical records. Computer vision assists in analyzing medical images \n",
    "like X-rays and MRIs. These technologies are improving patient outcomes and reducing healthcare costs.\n",
    "\"\"\"\n",
    "\n",
    "summary = summarizer(article, max_length=50, min_length=20, do_sample=False)\n",
    "print(\"Summarization:\")\n",
    "print(summary[0]['summary_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question Answering\n",
    "qa_pipeline = pipeline(\"question-answering\", model=\"distilbert-base-cased-distilled-squad\")\n",
    "\n",
    "context = \"The Eiffel Tower is located in Paris, France. It was completed in 1889 and stands 330 meters tall.\"\n",
    "questions = [\n",
    "    \"Where is the Eiffel Tower?\",\n",
    "    \"When was it completed?\",\n",
    "    \"How tall is it?\"\n",
    "]\n",
    "\n",
    "print(\"\\nQuestion Answering:\")\n",
    "for question in questions:\n",
    "    result = qa_pipeline(question=question, context=context)\n",
    "    print(f\"  Q: {question}\")\n",
    "    print(f\"  A: {result['answer']} (confidence: {result['score']:.4f})\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Named Entity Recognition\n",
    "ner = pipeline(\"ner\", model=\"dbmdz/bert-large-cased-finetuned-conll03-english\", aggregation_strategy=\"simple\")\n",
    "\n",
    "text = \"Microsoft was founded by Bill Gates and Paul Allen in Seattle.\"\n",
    "entities = ner(text)\n",
    "\n",
    "print(\"Named Entity Recognition:\")\n",
    "for entity in entities:\n",
    "    print(f\"  {entity['word']}: {entity['entity_group']} (score: {entity['score']:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Prompt Engineering Basics\n",
    "\n",
    "Effective prompting is crucial for getting good results from LLMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_prompts(prompts, text):\n",
    "    \"\"\"Compare different prompt strategies.\"\"\"\n",
    "    client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "    \n",
    "    for i, prompt in enumerate(prompts, 1):\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": prompt.format(text=text)}\n",
    "            ],\n",
    "            temperature=0\n",
    "        )\n",
    "        print(f\"\\nPrompt {i}:\")\n",
    "        print(f\"{prompt.format(text=text)}\")\n",
    "        print(f\"\\nResponse:\")\n",
    "        print(response.choices[0].message.content)\n",
    "        print(\"-\" * 80)\n",
    "\n",
    "text = \"The movie had great visuals but the plot was confusing and dragged on.\"\n",
    "\n",
    "prompts = [\n",
    "    # Bad prompt - vague\n",
    "    \"What do you think about: {text}\",\n",
    "    \n",
    "    # Better prompt - specific task\n",
    "    \"Analyze the sentiment of this movie review: {text}\",\n",
    "    \n",
    "    # Best prompt - specific task with format\n",
    "    \"\"\"Analyze the sentiment of this movie review: {text}\n",
    "    \n",
    "Provide:\n",
    "1. Overall sentiment (positive/negative/mixed)\n",
    "2. Positive aspects\n",
    "3. Negative aspects\n",
    "4. Sentiment score (0-10)\"\"\"\n",
    "]\n",
    "\n",
    "compare_prompts(prompts, text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Practice Exercises\n",
    "\n",
    "### Exercise 1: Text Preprocessing Pipeline\n",
    "Create a complete preprocessing function that takes raw text and returns cleaned, tokenized text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_pipeline(text, remove_stopwords=True, lemmatize=True):\n",
    "    \"\"\"Complete preprocessing pipeline.\"\"\"\n",
    "    # Your code here\n",
    "    pass\n",
    "\n",
    "# Test your function\n",
    "test_text = \"RT @user: Check out this AMAZING article! https://example.com #NLP #AI ðŸš€\"\n",
    "# result = preprocess_pipeline(test_text)\n",
    "# print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Zero-Shot Multi-Class Classification\n",
    "Use zero-shot classification to categorize news articles into topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = [\n",
    "    \"The stock market reached record highs today as investors reacted positively to earnings reports.\",\n",
    "    \"Scientists discovered a new species of marine life in the deep ocean.\",\n",
    "    \"The championship game went into overtime with an exciting finish.\"\n",
    "]\n",
    "\n",
    "# Use any method (OpenAI, Claude, or Hugging Face) to classify these articles\n",
    "# Categories: business, science, sports, politics, entertainment\n",
    "\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Prompt Engineering\n",
    "Design prompts for extracting structured information from unstructured text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_posting = \"\"\"\n",
    "We are looking for a Senior Data Scientist with 5+ years of experience in ML and NLP.\n",
    "Must have Python, TensorFlow, and PyTorch skills. Salary: $120k-$180k.\n",
    "Location: San Francisco, CA (Hybrid). Apply by December 31, 2024.\n",
    "\"\"\"\n",
    "\n",
    "# Create a prompt to extract: position, experience, skills, salary, location, deadline\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, you learned:\n",
    "1. Common NLP tasks and their applications\n",
    "2. Traditional text preprocessing techniques\n",
    "3. Modern tokenization with transformer models\n",
    "4. Zero-shot inference using OpenAI, Anthropic, and Hugging Face\n",
    "5. Prompt engineering basics for better results\n",
    "\n",
    "**Next Steps**: Practice with Quiz 1 and prepare for the live session on zero-shot inference!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
