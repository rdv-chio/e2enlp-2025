{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 1: Introduction to NLP\n",
    "\n",
    "This notebook covers:\n",
    "- NLP Tasks Overview\n",
    "- Data Preprocessing\n",
    "- Tokenization\n",
    "- Prompting and Zero-shot Inference\n",
    "\n",
    "**Prerequisites**: Install required packages:\n",
    "```bash\n",
    "pip install openai anthropic transformers datasets nltk spacy\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Common NLP Tasks\n",
    "\n",
    "Natural Language Processing encompasses various tasks:\n",
    "\n",
    "1. **Text Classification**: Categorizing text into predefined classes\n",
    "2. **Named Entity Recognition (NER)**: Identifying entities like names, locations, organizations\n",
    "3. **Sentiment Analysis**: Determining the emotional tone of text\n",
    "4. **Question Answering**: Extracting answers from context\n",
    "5. **Text Summarization**: Creating concise summaries\n",
    "6. **Machine Translation**: Converting text between languages\n",
    "7. **Text Generation**: Creating coherent text based on prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preprocessing\n",
    "\n",
    "Preprocessing is essential for preparing text data for NLP models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text:\n",
      "Natural Language Processing (NLP) is AMAZING! It helps computers understand human language. #AI #ML\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('averaged_perceptron_tagger', quiet=True)\n",
    "nltk.download('punkt_tab', quiet=True)\n",
    "\n",
    "# Sample text\n",
    "text = \"Natural Language Processing (NLP) is AMAZING! It helps computers understand human language. #AI #ML\"\n",
    "\n",
    "print(\"Original text:\")\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1. Lowercased:\n",
      "natural language processing (nlp) is amazing! it helps computers understand human language. #ai #ml\n"
     ]
    }
   ],
   "source": [
    "# 1. Lowercasing\n",
    "text_lower = text.lower()\n",
    "print(\"\\n1. Lowercased:\")\n",
    "print(text_lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2. Cleaned text:\n",
      "natural language processing nlp is amazing it helps computers understand human language  \n"
     ]
    }
   ],
   "source": [
    "# 2. Remove URLs, mentions, and hashtags\n",
    "def clean_text(text):\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    # Remove mentions and hashtags\n",
    "    text = re.sub(r'@\\w+|#\\w+', '', text)\n",
    "    # Remove special characters and numbers\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    return text\n",
    "\n",
    "text_clean = clean_text(text_lower)\n",
    "print(\"\\n2. Cleaned text:\")\n",
    "print(text_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "3. Tokens:\n",
      "['natural', 'language', 'processing', 'nlp', 'is', 'amazing', 'it', 'helps', 'computers', 'understand', 'human', 'language']\n"
     ]
    }
   ],
   "source": [
    "# 3. Tokenization\n",
    "tokens = word_tokenize(text_clean)\n",
    "print(\"\\n3. Tokens:\")\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "4. Tokens without stopwords:\n",
      "['natural', 'language', 'processing', 'nlp', 'amazing', 'helps', 'computers', 'understand', 'human', 'language']\n"
     ]
    }
   ],
   "source": [
    "# 4. Remove stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "print(\"\\n4. Tokens without stopwords:\")\n",
    "print(filtered_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5. Stemmed tokens:\n",
      "['natur', 'languag', 'process', 'nlp', 'amaz', 'help', 'comput', 'understand', 'human', 'languag']\n"
     ]
    }
   ],
   "source": [
    "# 5. Stemming\n",
    "stemmer = PorterStemmer()\n",
    "stemmed = [stemmer.stem(word) for word in filtered_tokens]\n",
    "print(\"\\n5. Stemmed tokens:\")\n",
    "print(stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "6. Lemmatized tokens:\n",
      "['natural', 'language', 'processing', 'nlp', 'amazing', 'help', 'computer', 'understand', 'human', 'language']\n"
     ]
    }
   ],
   "source": [
    "# 6. Lemmatization (preferred over stemming)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized = [lemmatizer.lemmatize(word) for word in filtered_tokens]\n",
    "print(\"\\n6. Lemmatized tokens:\")\n",
    "print(lemmatized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Modern Tokenization with Transformers\n",
    "\n",
    "Modern NLP models use subword tokenization for better handling of rare words and morphology."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "bert-base-uncased:\n",
      "  Tokens: ['token', '##ization', 'is', 'fundamental', 'for', 'nl', '##p', '!']\n",
      "  Token IDs: [101, 19204, 3989, 2003, 8050, 2005, 17953, 2361, 999, 102]\n",
      "  Decoded: [CLS] tokenization is fundamental for nlp! [SEP]\n",
      "\n",
      "gpt2:\n",
      "  Tokens: ['Token', 'ization', 'ƒ†is', 'ƒ†fundamental', 'ƒ†for', 'ƒ†N', 'LP', '!']\n",
      "  Token IDs: [30642, 1634, 318, 7531, 329, 399, 19930, 0]\n",
      "  Decoded: Tokenization is fundamental for NLP!\n",
      "\n",
      "t5-small:\n",
      "  Tokens: ['‚ñÅTo', 'ken', 'ization', '‚ñÅis', '‚ñÅfundamental', '‚ñÅfor', '‚ñÅN', 'LP', '!']\n",
      "  Token IDs: [304, 2217, 1707, 19, 4431, 21, 445, 6892, 55, 1]\n",
      "  Decoded: Tokenization is fundamental for NLP!</s>\n",
      "\n",
      "facebook/bart-base:\n",
      "  Tokens: ['Token', 'ization', 'ƒ†is', 'ƒ†fundamental', 'ƒ†for', 'ƒ†N', 'LP', '!']\n",
      "  Token IDs: [0, 45643, 1938, 16, 6451, 13, 234, 21992, 328, 2]\n",
      "  Decoded: <s>Tokenization is fundamental for NLP!</s>\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Different tokenizers for different models\n",
    "models = [\n",
    "    \"bert-base-uncased\",\n",
    "    \"gpt2\",\n",
    "    \"t5-small\",\n",
    "    \"facebook/bart-base\"\n",
    "]\n",
    "\n",
    "sample_text = \"Tokenization is fundamental for NLP!\"\n",
    "\n",
    "for model_name in models:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    tokens = tokenizer.tokenize(sample_text)\n",
    "    token_ids = tokenizer.encode(sample_text)\n",
    "    \n",
    "    print(f\"\\n{model_name}:\")\n",
    "    print(f\"  Tokens: {tokens}\")\n",
    "    print(f\"  Token IDs: {token_ids}\")\n",
    "    print(f\"  Decoded: {tokenizer.decode(token_ids)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Special Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Special tokens:\n",
      "  PAD token: [PAD] (ID: 0)\n",
      "  CLS token: [CLS] (ID: 101)\n",
      "  SEP token: [SEP] (ID: 102)\n",
      "  UNK token: [UNK] (ID: 100)\n",
      "  Vocab size: 30522\n",
      "\n",
      "Encoded output:\n",
      "  Input IDs: [101, 7592, 2088, 999, 102, 0, 0, 0, 0, 0]\n",
      "  Attention Mask: [1, 1, 1, 1, 1, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "print(\"Special tokens:\")\n",
    "print(f\"  PAD token: {tokenizer.pad_token} (ID: {tokenizer.pad_token_id})\")\n",
    "print(f\"  CLS token: {tokenizer.cls_token} (ID: {tokenizer.cls_token_id})\")\n",
    "print(f\"  SEP token: {tokenizer.sep_token} (ID: {tokenizer.sep_token_id})\")\n",
    "print(f\"  UNK token: {tokenizer.unk_token} (ID: {tokenizer.unk_token_id})\")\n",
    "print(f\"  Vocab size: {tokenizer.vocab_size}\")\n",
    "\n",
    "# Tokenize with special tokens\n",
    "encoded = tokenizer.encode_plus(\n",
    "    \"Hello world!\",\n",
    "    add_special_tokens=True,\n",
    "    return_attention_mask=True,\n",
    "    padding='max_length',\n",
    "    max_length=10\n",
    ")\n",
    "\n",
    "print(\"\\nEncoded output:\")\n",
    "print(f\"  Input IDs: {encoded['input_ids']}\")\n",
    "print(f\"  Attention Mask: {encoded['attention_mask']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Prompting and Zero-Shot Inference\n",
    "\n",
    "Modern LLMs can perform tasks without training by using carefully crafted prompts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Zero-Shot Text Classification with OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment Classification (OpenAI):\n",
      "  Text: I love this product! It's amazing!\n",
      "  Sentiment: positive\n",
      "\n",
      "  Text: This is terrible. I want my money back.\n",
      "  Sentiment: negative\n",
      "\n",
      "  Text: The product arrived on time.\n",
      "  Sentiment: positive\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize OpenAI client (requires OPENAI_API_KEY environment variable)\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "def classify_sentiment_openai(text):\n",
    "    \"\"\"Classify sentiment using zero-shot prompting with OpenAI.\"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a sentiment analysis expert. Classify the sentiment as positive, negative, or neutral.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"Classify the sentiment of this text: '{text}'\\n\\nRespond with only one word: positive, negative, or neutral.\"}\n",
    "        ],\n",
    "        temperature=0\n",
    "    )\n",
    "    return response.choices[0].message.content.strip().lower()\n",
    "\n",
    "# Test examples\n",
    "examples = [\n",
    "    \"I love this product! It's amazing!\",\n",
    "    \"This is terrible. I want my money back.\",\n",
    "    \"The product arrived on time.\"\n",
    "]\n",
    "\n",
    "print(\"Sentiment Classification (OpenAI):\")\n",
    "for text in examples:\n",
    "    sentiment = classify_sentiment_openai(text)\n",
    "    print(f\"  Text: {text}\")\n",
    "    print(f\"  Sentiment: {sentiment}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Zero-Shot with Anthropic Claude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import anthropic\n",
    "from dotenv import load_dotenv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Named Entity Recognition (Claude):\n",
      "```json\n",
      "{\n",
      "  \"people\": [\n",
      "    \"Tim Cook\",\n",
      "    \"Elon Musk\"\n",
      "  ],\n",
      "  \"organizations\": [\n",
      "    \"Apple Inc.\",\n",
      "    \"Tesla\"\n",
      "  ],\n",
      "  \"locations\": [\n",
      "    \"Cupertino\",\n",
      "    \"California\"\n",
      "  ]\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "import anthropic\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize Anthropic client (requires ANTHROPIC_API_KEY environment variable)\n",
    "client_anthropic = anthropic.Anthropic(api_key=os.getenv(\"ANTHROPIC_API_KEY\"))\n",
    "\n",
    "def extract_entities_claude(text):\n",
    "    \"\"\"Extract named entities using Claude.\"\"\"\n",
    "    message = client_anthropic.messages.create(\n",
    "        model=\"claude-sonnet-4-5-20250929\",\n",
    "        max_tokens=1024,\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": f\"Extract all named entities (people, organizations, locations) from this text: '{text}'\\n\\nProvide the response as a JSON object with keys: people, organizations, locations.\"}\n",
    "        ]\n",
    "    )\n",
    "    return message.content[0].text\n",
    "\n",
    "text = \"Apple Inc. CEO Tim Cook announced new products in Cupertino, California. Elon Musk from Tesla also attended.\"\n",
    "\n",
    "print(\"Named Entity Recognition (Claude):\")\n",
    "print(extract_entities_claude(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Zero-Shot with Hugging Face Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero-Shot Classification (Hugging Face):\n",
      "Text: I am excited to learn about natural language processing!\n",
      "\n",
      "  technology: 0.8605\n",
      "  education: 0.1059\n",
      "  sports: 0.0197\n",
      "  politics: 0.0139\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Zero-shot classification pipeline\n",
    "classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
    "\n",
    "text = \"I am excited to learn about natural language processing!\"\n",
    "candidate_labels = [\"education\", \"technology\", \"sports\", \"politics\"]\n",
    "\n",
    "result = classifier(text, candidate_labels)\n",
    "\n",
    "print(\"Zero-Shot Classification (Hugging Face):\")\n",
    "print(f\"Text: {text}\\n\")\n",
    "for label, score in zip(result['labels'], result['scores']):\n",
    "    print(f\"  {label}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Multiple NLP Tasks with Zero-Shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31f5064a0e804966a5770395609752da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c20a976f9e20417d9fc1ae0ff477935a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.63G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "121b9115d3e14c619279cacdee5ecfe7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a82d150374e84a3ba66b5a22225c0f2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2b4265454eb4863a8369700bc220ec8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95673294415d4129b643b83b9444a300",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarization:\n",
      "Artificial intelligence is transforming healthcare in unprecedented ways. Machine learning algorithms can now detect diseases earlier and more accurately than traditional methods. Natural language processing helps doctors extract insights from medical records. Computer vision assists in analyzing medical images like\n"
     ]
    }
   ],
   "source": [
    "# Summarization\n",
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "\n",
    "article = \"\"\"\n",
    "Artificial intelligence is transforming healthcare in unprecedented ways. Machine learning algorithms \n",
    "can now detect diseases earlier and more accurately than traditional methods. Natural language processing \n",
    "helps doctors extract insights from medical records. Computer vision assists in analyzing medical images \n",
    "like X-rays and MRIs. These technologies are improving patient outcomes and reducing healthcare costs.\n",
    "\"\"\"\n",
    "\n",
    "summary = summarizer(article, max_length=50, min_length=20, do_sample=False)\n",
    "print(\"Summarization:\")\n",
    "print(summary[0]['summary_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4860f7a2e3542638f9b8f406952911e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/473 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3db484b256a64c5d86ca16440dcfbe68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/261M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96288270700b436cad7245add6b7bfd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea6ca2eeb8d448eeb9bb9ebbdd0ca900",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d510caea6c442f49ee0d5b7e6f38149",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question Answering:\n",
      "  Q: Where is the Eiffel Tower?\n",
      "  A: Paris, France (confidence: 0.9292)\n",
      "\n",
      "  Q: When was it completed?\n",
      "  A: 1889 (confidence: 0.9777)\n",
      "\n",
      "  Q: How tall is it?\n",
      "  A: 330 meters (confidence: 0.6323)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Question Answering\n",
    "qa_pipeline = pipeline(\"question-answering\", model=\"distilbert-base-cased-distilled-squad\")\n",
    "\n",
    "context = \"The Eiffel Tower is located in Paris, France. It was completed in 1889 and stands 330 meters tall.\"\n",
    "questions = [\n",
    "    \"Where is the Eiffel Tower?\",\n",
    "    \"When was it completed?\",\n",
    "    \"How tall is it?\"\n",
    "]\n",
    "\n",
    "print(\"\\nQuestion Answering:\")\n",
    "for question in questions:\n",
    "    result = qa_pipeline(question=question, context=context)\n",
    "    print(f\"  Q: {question}\")\n",
    "    print(f\"  A: {result['answer']} (confidence: {result['score']:.4f})\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17c691f69769436593f2375f593d0a50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/998 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93aef0a09a9c4d67b30f7fbb02cec234",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.33G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "494757e156bd4f24b5729931864c2c64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/60.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f7359b758454f1faae85e9322443a4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Named Entity Recognition:\n",
      "  Microsoft: ORG (score: 0.9996)\n",
      "  Bill Gates: PER (score: 0.9918)\n",
      "  Paul Allen: PER (score: 0.9983)\n",
      "  Seattle: LOC (score: 0.9982)\n"
     ]
    }
   ],
   "source": [
    "# Named Entity Recognition\n",
    "ner = pipeline(\"ner\", model=\"dbmdz/bert-large-cased-finetuned-conll03-english\", aggregation_strategy=\"simple\")\n",
    "\n",
    "text = \"Microsoft was founded by Bill Gates and Paul Allen in Seattle.\"\n",
    "entities = ner(text)\n",
    "\n",
    "print(\"Named Entity Recognition:\")\n",
    "for entity in entities:\n",
    "    print(f\"  {entity['word']}: {entity['entity_group']} (score: {entity['score']:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Prompt Engineering Basics\n",
    "\n",
    "Effective prompting is crucial for getting good results from LLMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prompt 1:\n",
      "What do you think about: The movie had great visuals but the plot was confusing and dragged on.\n",
      "\n",
      "Response:\n",
      "It sounds like you found the movie to be a mixed experience. Great visuals can certainly enhance a film and make it more engaging, but if the plot is confusing and feels drawn out, it can detract from the overall enjoyment. A strong narrative is often essential for connecting with the audience, so it's understandable that a convoluted plot could overshadow the visual appeal. Did any specific scenes or elements stand out to you, either positively or negatively?\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Prompt 2:\n",
      "Analyze the sentiment of this movie review: The movie had great visuals but the plot was confusing and dragged on.\n",
      "\n",
      "Response:\n",
      "The sentiment of the movie review is mixed. The reviewer expresses a positive sentiment towards the visuals, indicating that they were impressive or well-done. However, this is contrasted with a negative sentiment regarding the plot, which the reviewer found confusing and slow-paced. Overall, the review highlights both positive and negative aspects of the movie, leading to a nuanced sentiment.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Prompt 3:\n",
      "Analyze the sentiment of this movie review: The movie had great visuals but the plot was confusing and dragged on.\n",
      "\n",
      "Provide:\n",
      "1. Overall sentiment (positive/negative/mixed)\n",
      "2. Positive aspects\n",
      "3. Negative aspects\n",
      "4. Sentiment score (0-10)\n",
      "\n",
      "Response:\n",
      "1. **Overall sentiment**: Mixed\n",
      "\n",
      "2. **Positive aspects**: \n",
      "   - Great visuals\n",
      "\n",
      "3. **Negative aspects**: \n",
      "   - Confusing plot\n",
      "   - Plot dragged on\n",
      "\n",
      "4. **Sentiment score**: 5/10\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def compare_prompts(prompts, text):\n",
    "    \"\"\"Compare different prompt strategies.\"\"\"\n",
    "    client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "    \n",
    "    for i, prompt in enumerate(prompts, 1):\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": prompt.format(text=text)}\n",
    "            ],\n",
    "            temperature=0\n",
    "        )\n",
    "        print(f\"\\nPrompt {i}:\")\n",
    "        print(f\"{prompt.format(text=text)}\")\n",
    "        print(f\"\\nResponse:\")\n",
    "        print(response.choices[0].message.content)\n",
    "        print(\"-\" * 80)\n",
    "\n",
    "text = \"The movie had great visuals but the plot was confusing and dragged on.\"\n",
    "\n",
    "prompts = [\n",
    "    # Bad prompt - vague\n",
    "    \"What do you think about: {text}\",\n",
    "    \n",
    "    # Better prompt - specific task\n",
    "    \"Analyze the sentiment of this movie review: {text}\",\n",
    "    \n",
    "    # Best prompt - specific task with format\n",
    "    \"\"\"Analyze the sentiment of this movie review: {text}\n",
    "    \n",
    "Provide:\n",
    "1. Overall sentiment (positive/negative/mixed)\n",
    "2. Positive aspects\n",
    "3. Negative aspects\n",
    "4. Sentiment score (0-10)\"\"\"\n",
    "]\n",
    "\n",
    "compare_prompts(prompts, text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Practice Exercises\n",
    "\n",
    "### Exercise 1: Text Preprocessing Pipeline\n",
    "Create a complete preprocessing function that takes raw text and returns cleaned, tokenized text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text:\n",
      "RT @user: Check out this AMAZING article! https://example.com #NLP #AI üöÄ\n",
      "\n",
      "Preprocessed tokens:\n",
      "['check', 'amazing', 'article']\n",
      "\n",
      "================================================================================\n",
      "With stopwords kept:\n",
      "['check', 'out', 'this', 'amazing', 'article']\n",
      "\n",
      "================================================================================\n",
      "With stemming instead of lemmatization:\n",
      "['check', 'amaz', 'articl']\n"
     ]
    }
   ],
   "source": [
    "def preprocess_pipeline(text, remove_stopwords=True, lemmatize=True):\n",
    "    \"\"\"\n",
    "    Complete preprocessing pipeline for text data.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Raw input text\n",
    "        remove_stopwords (bool): Whether to remove stopwords\n",
    "        lemmatize (bool): Whether to lemmatize (True) or stem (False)\n",
    "    \n",
    "    Returns:\n",
    "        list: Cleaned and tokenized text\n",
    "    \"\"\"\n",
    "    # Your code here\n",
    "    # 1. Lowercase the text\n",
    "    text_lower = text.lower()\n",
    "    \n",
    "    # 2. Remove URLs\n",
    "    text_clean = re.sub(r'http\\S+|www\\S+|https\\S+', '', text_lower, flags=re.MULTILINE)\n",
    "    \n",
    "    # 3. Remove mentions (@user) and hashtags (#tag)\n",
    "    text_clean = re.sub(r'@\\w+', '', text_clean)\n",
    "    text_clean = re.sub(r'#\\w+', '', text_clean)\n",
    "    \n",
    "    # 4. Remove RT (retweet indicator)\n",
    "    text_clean = re.sub(r'\\brt\\b', '', text_clean)\n",
    "    \n",
    "    # 5. Remove special characters, numbers, and punctuation\n",
    "    text_clean = re.sub(r'[^a-zA-Z\\s]', '', text_clean)\n",
    "    \n",
    "    # 6. Remove extra whitespace\n",
    "    text_clean = ' '.join(text_clean.split())\n",
    "    \n",
    "    # 7. Tokenization\n",
    "    tokens = word_tokenize(text_clean)\n",
    "    \n",
    "    # 8. Remove stopwords (optional)\n",
    "    if remove_stopwords:\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "    else:\n",
    "        filtered_tokens = tokens\n",
    "    \n",
    "    # 9. Lemmatization or Stemming\n",
    "    if lemmatize:\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        lemmatized = [lemmatizer.lemmatize(word) for word in filtered_tokens]\n",
    "        final_tokens = lemmatized\n",
    "    else:\n",
    "        stemmer = PorterStemmer()\n",
    "        stemmed = [stemmer.stem(word) for word in filtered_tokens]\n",
    "        final_tokens = stemmed\n",
    "    \n",
    "    # 10. Remove empty strings and single characters (optional)\n",
    "    final_tokens = [word for word in final_tokens if len(word) > 1]\n",
    "    \n",
    "    return final_tokens\n",
    "\n",
    "# Test your function\n",
    "test_text = \"RT @user: Check out this AMAZING article! https://example.com #NLP #AI üöÄ\"\n",
    "result = preprocess_pipeline(test_text)\n",
    "print(\"Original text:\")\n",
    "print(test_text)\n",
    "print(\"\\nPreprocessed tokens:\")\n",
    "print(result)\n",
    "\n",
    "# Test with different options\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"With stopwords kept:\")\n",
    "result_with_stopwords = preprocess_pipeline(test_text, remove_stopwords=False)\n",
    "print(result_with_stopwords)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"With stemming instead of lemmatization:\")\n",
    "result_stemmed = preprocess_pipeline(test_text, lemmatize=False)\n",
    "print(result_stemmed)\n",
    "\n",
    "\n",
    "# Test your function\n",
    "test_text = \"RT @user: Check out this AMAZING article! https://example.com #NLP #AI üöÄ\"\n",
    "# result = preprocess_pipeline(test_text)\n",
    "# print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Zero-Shot Multi-Class Classification\n",
    "Use zero-shot classification to categorize news articles into topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import anthropic\n",
    "from transformers import pipeline\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero-Shot Multi-Class Classification Results:\n",
      "================================================================================\n",
      "\n",
      "Article 1:\n",
      "Text: The stock market reached record highs today as investors reacted positively to earnings reports.\n",
      "\n",
      "Predicted Category: business (confidence: 0.6444)\n",
      "\n",
      "All scores:\n",
      "  business: 0.6444\n",
      "  entertainment: 0.1830\n",
      "  science: 0.0744\n",
      "  sports: 0.0572\n",
      "  politics: 0.0410\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Article 2:\n",
      "Text: Scientists discovered a new species of marine life in the deep ocean.\n",
      "\n",
      "Predicted Category: science (confidence: 0.9729)\n",
      "\n",
      "All scores:\n",
      "  science: 0.9729\n",
      "  entertainment: 0.0121\n",
      "  business: 0.0071\n",
      "  sports: 0.0050\n",
      "  politics: 0.0028\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Article 3:\n",
      "Text: The championship game went into overtime with an exciting finish.\n",
      "\n",
      "Predicted Category: sports (confidence: 0.6168)\n",
      "\n",
      "All scores:\n",
      "  sports: 0.6168\n",
      "  entertainment: 0.3617\n",
      "  business: 0.0136\n",
      "  science: 0.0044\n",
      "  politics: 0.0036\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Using OpenAI API:\n",
      "================================================================================\n",
      "\n",
      "Article 1:\n",
      "Text: The stock market reached record highs today as investors reacted positively to earnings reports.\n",
      "Predicted Category: business\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Article 2:\n",
      "Text: Scientists discovered a new species of marine life in the deep ocean.\n",
      "Predicted Category: science\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Article 3:\n",
      "Text: The championship game went into overtime with an exciting finish.\n",
      "Predicted Category: sports\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Using Anthropic Claude:\n",
      "================================================================================\n",
      "\n",
      "Article 1:\n",
      "Text: The stock market reached record highs today as investors reacted positively to earnings reports.\n",
      "Classification: business\n",
      "\n",
      "Confidence: 0.98\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Article 2:\n",
      "Text: Scientists discovered a new species of marine life in the deep ocean.\n",
      "Classification: science\n",
      "\n",
      "Confidence: 1.0\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Article 3:\n",
      "Text: The championship game went into overtime with an exciting finish.\n",
      "Classification: sports\n",
      "\n",
      "Confidence: 0.99\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Bonus: Detailed Multi-Label Analysis\n",
      "================================================================================\n",
      "\n",
      "Article 1:\n",
      "Text: The stock market reached record highs today as investors reacted positively to earnings reports.\n",
      "\n",
      "Detailed Analysis:\n",
      "```json\n",
      "{\n",
      "  \"primary_category\": \"business\",\n",
      "  \"confidence_score\": 0.95,\n",
      "  \"reasoning\": \"The article discusses the stock market and investor reactions, which are key topics in the business sector.\",\n",
      "  \"secondary_category\": null\n",
      "}\n",
      "```\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Article 2:\n",
      "Text: Scientists discovered a new species of marine life in the deep ocean.\n",
      "\n",
      "Detailed Analysis:\n",
      "```json\n",
      "{\n",
      "  \"primary_category\": \"science\",\n",
      "  \"confidence_score\": 0.95,\n",
      "  \"reasoning\": \"The article discusses a scientific discovery related to marine life, which falls under the category of science.\",\n",
      "  \"secondary_category\": null\n",
      "}\n",
      "```\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Article 3:\n",
      "Text: The championship game went into overtime with an exciting finish.\n",
      "\n",
      "Detailed Analysis:\n",
      "```json\n",
      "{\n",
      "  \"primary_category\": \"sports\",\n",
      "  \"confidence_score\": 0.9,\n",
      "  \"reasoning\": \"The mention of a championship game and overtime indicates a focus on competitive athletic events, which is characteristic of the sports category.\",\n",
      "  \"secondary_category\": null\n",
      "}\n",
      "```\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "articles = [\n",
    "    \"The stock market reached record highs today as investors reacted positively to earnings reports.\",\n",
    "    \"Scientists discovered a new species of marine life in the deep ocean.\",\n",
    "    \"The championship game went into overtime with an exciting finish.\"\n",
    "]\n",
    "\n",
    "# Use any method (OpenAI, Claude, or Hugging Face) to classify these articles\n",
    "# Categories: business, science, sports, politics, entertainment\n",
    "\n",
    "# Your code here\n",
    "# Solution 1: Using Hugging Face Transformers (Recommended for this task)\n",
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
    "candidate_labels = [\"business\", \"science\", \"sports\", \"politics\", \"entertainment\"]\n",
    "\n",
    "print(\"Zero-Shot Multi-Class Classification Results:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for i, article in enumerate(articles, 1):\n",
    "    result = classifier(article, candidate_labels)\n",
    "    \n",
    "    print(f\"\\nArticle {i}:\")\n",
    "    print(f\"Text: {article}\")\n",
    "    print(f\"\\nPredicted Category: {result['labels'][0]} (confidence: {result['scores'][0]:.4f})\")\n",
    "    print(\"\\nAll scores:\")\n",
    "    for label, score in zip(result['labels'], result['scores']):\n",
    "        print(f\"  {label}: {score:.4f}\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "# Solution 2: Using OpenAI (Alternative approach)\n",
    "print(\"\\n\\nUsing OpenAI API:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "def classify_article_openai(article, categories):\n",
    "    \"\"\"Classify article using OpenAI.\"\"\"\n",
    "    client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a text classification expert. Classify articles into one of the given categories.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"\"\"Classify this article into ONE of these categories: {', '.join(categories)}\n",
    "\n",
    "Article: {article}\n",
    "\n",
    "Respond with only the category name.\"\"\"}\n",
    "        ],\n",
    "        temperature=0\n",
    "    )\n",
    "    return response.choices[0].message.content.strip().lower()\n",
    "\n",
    "for i, article in enumerate(articles, 1):\n",
    "    category = classify_article_openai(article, candidate_labels)\n",
    "    print(f\"\\nArticle {i}:\")\n",
    "    print(f\"Text: {article}\")\n",
    "    print(f\"Predicted Category: {category}\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "# Solution 3: Using Anthropic Claude (Alternative approach)\n",
    "print(\"\\n\\nUsing Anthropic Claude:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "def classify_article_claude(article, categories):\n",
    "    \"\"\"Classify article using Claude.\"\"\"\n",
    "    client_anthropic = anthropic.Anthropic(api_key=os.getenv(\"ANTHROPIC_API_KEY\"))\n",
    "    \n",
    "    message = client_anthropic.messages.create(\n",
    "        model=\"claude-sonnet-4-5-20250929\",\n",
    "        max_tokens=100,\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": f\"\"\"Classify this article into ONE of these categories: {', '.join(categories)}\n",
    "\n",
    "Article: {article}\n",
    "\n",
    "Respond with only the category name and a confidence score (0-1).\"\"\"}\n",
    "        ]\n",
    "    )\n",
    "    return message.content[0].text\n",
    "\n",
    "for i, article in enumerate(articles, 1):\n",
    "    result = classify_article_claude(article, candidate_labels)\n",
    "    print(f\"\\nArticle {i}:\")\n",
    "    print(f\"Text: {article}\")\n",
    "    print(f\"Classification: {result}\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "# Bonus: Batch classification with detailed analysis\n",
    "print(\"\\n\\nBonus: Detailed Multi-Label Analysis\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "def detailed_classification(article):\n",
    "    \"\"\"Get detailed classification with reasoning.\"\"\"\n",
    "    client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a text classification expert.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"\"\"Classify this article and provide reasoning.\n",
    "\n",
    "Article: {article}\n",
    "\n",
    "Categories: business, science, sports, politics, entertainment\n",
    "\n",
    "Provide:\n",
    "1. Primary category\n",
    "2. Confidence score (0-1)\n",
    "3. Brief reasoning (one sentence)\n",
    "4. Secondary category (if applicable)\n",
    "\n",
    "Format as JSON.\"\"\"}\n",
    "        ],\n",
    "        temperature=0\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "for i, article in enumerate(articles, 1):\n",
    "    result = detailed_classification(article)\n",
    "    print(f\"\\nArticle {i}:\")\n",
    "    print(f\"Text: {article}\")\n",
    "    print(f\"\\nDetailed Analysis:\")\n",
    "    print(result)\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Prompt Engineering\n",
    "Design prompts for extracting structured information from unstructured text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "import anthropic\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solution 1: OpenAI with Structured JSON Extraction\n",
      "================================================================================\n",
      "\n",
      "Extracted Information:\n",
      "{\n",
      "  \"position\": \"Senior Data Scientist\",\n",
      "  \"experience\": \"5+ years\",\n",
      "  \"skills\": [\n",
      "    \"Python\",\n",
      "    \"TensorFlow\",\n",
      "    \"PyTorch\"\n",
      "  ],\n",
      "  \"salary\": \"$120k-$180k\",\n",
      "  \"location\": \"San Francisco, CA (Hybrid)\",\n",
      "  \"deadline\": \"December 31, 2024\"\n",
      "}\n",
      "\n",
      "Parsed JSON:\n",
      "{\n",
      "  \"position\": \"Senior Data Scientist\",\n",
      "  \"experience\": \"5+ years\",\n",
      "  \"skills\": [\n",
      "    \"Python\",\n",
      "    \"TensorFlow\",\n",
      "    \"PyTorch\"\n",
      "  ],\n",
      "  \"salary\": \"$120k-$180k\",\n",
      "  \"location\": \"San Francisco, CA (Hybrid)\",\n",
      "  \"deadline\": \"December 31, 2024\"\n",
      "}\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Solution 2: Anthropic Claude with Detailed Extraction\n",
      "================================================================================\n",
      "\n",
      "Extracted Information:\n",
      "```json\n",
      "{\n",
      "  \"position\": \"Senior Data Scientist\",\n",
      "  \"experience\": \"5+ years\",\n",
      "  \"skills\": [\"ML\", \"NLP\", \"Python\", \"TensorFlow\", \"PyTorch\"],\n",
      "  \"salary\": \"$120k-$180k\",\n",
      "  \"location\": \"San Francisco, CA (Hybrid)\",\n",
      "  \"deadline\": \"December 31, 2024\"\n",
      "}\n",
      "```\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Solution 3: Multi-Step Extraction with Validation\n",
      "================================================================================\n",
      "\n",
      "Step 1 - Extraction:\n",
      "```json\n",
      "{\n",
      "  \"position\": {\n",
      "    \"value\": \"Senior Data Scientist\",\n",
      "    \"confidence\": 1.0\n",
      "  },\n",
      "  \"experience\": {\n",
      "    \"value\": \"5+ years\",\n",
      "    \"confidence\": 1.0\n",
      "  },\n",
      "  \"skills\": {\n",
      "    \"value\": [\"Python\", \"TensorFlow\", \"PyTorch\"],\n",
      "    \"confidence\": 1.0\n",
      "  },\n",
      "  \"salary\": {\n",
      "    \"value\": \"$120k-$180k\",\n",
      "    \"confidence\": 1.0\n",
      "  },\n",
      "  \"location\": {\n",
      "    \"value\": \"San Francisco, CA (Hybrid)\",\n",
      "    \"confidence\": 1.0\n",
      "  },\n",
      "  \"deadline\": {\n",
      "    \"value\": \"December 31, 2024\",\n",
      "    \"confidence\": 1.0\n",
      "  }\n",
      "}\n",
      "```\n",
      "\n",
      "Step 2 - Validation:\n",
      "The extracted job information appears to be accurate based on the original job posting. Here‚Äôs the validation:\n",
      "\n",
      "1. **Position**: Correctly extracted as \"Senior Data Scientist\".\n",
      "2. **Experience**: Correctly extracted as \"5+ years\".\n",
      "3. **Skills**: Correctly extracted as [\"Python\", \"TensorFlow\", \"PyTorch\"].\n",
      "4. **Salary**: Correctly extracted as \"$120k-$180k\".\n",
      "5. **Location**: Correctly extracted as \"San Francisco, CA (Hybrid)\".\n",
      "6. **Deadline**: Correctly extracted as \"December 31, 2024\".\n",
      "\n",
      "Since all fields match the original job posting without any discrepancies, no corrections are needed.\n",
      "\n",
      "### Corrected JSON:\n",
      "```json\n",
      "{\n",
      "  \"position\": {\n",
      "    \"value\": \"Senior Data Scientist\",\n",
      "    \"confidence\": 1.0\n",
      "  },\n",
      "  \"experience\": {\n",
      "    \"value\": \"5+ years\",\n",
      "    \"confidence\": 1.0\n",
      "  },\n",
      "  \"skills\": {\n",
      "    \"value\": [\"Python\", \"TensorFlow\", \"PyTorch\"],\n",
      "    \"confidence\": 1.0\n",
      "  },\n",
      "  \"salary\": {\n",
      "    \"value\": \"$120k-$180k\",\n",
      "    \"confidence\": 1.0\n",
      "  },\n",
      "  \"location\": {\n",
      "    \"value\": \"San Francisco, CA (Hybrid)\",\n",
      "    \"confidence\": 1.0\n",
      "  },\n",
      "  \"deadline\": {\n",
      "    \"value\": \"December 31, 2024\",\n",
      "    \"confidence\": 1.0\n",
      "  }\n",
      "}\n",
      "```\n",
      "\n",
      "### Notes on Corrections:\n",
      "- No corrections were made as the extracted information is accurate.\n",
      "\n",
      "### Overall Extraction Quality:\n",
      "- **High**: The extraction is complete and matches the original job posting perfectly.\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Solution 4: Comparing Different Prompt Strategies\n",
      "================================================================================\n",
      "\n",
      "Basic Prompt Strategy:\n",
      "----------------------------------------\n",
      "**Job Title:** Senior Data Scientist\n",
      "\n",
      "**Experience Required:** 5+ years in Machine Learning (ML) and Natural Language Processing (NLP)\n",
      "\n",
      "**Skills Required:**\n",
      "- Python\n",
      "- TensorFlow\n",
      "- PyTorch\n",
      "\n",
      "**Salary Range:** $120,000 - $180,000\n",
      "\n",
      "**Location:** San Francisco, CA (Hybrid)\n",
      "\n",
      "**Application Deadline:** December 31, 2024\n",
      "\n",
      "\n",
      "Structured Prompt Strategy:\n",
      "----------------------------------------\n",
      "Here are the extracted fields from the job posting:\n",
      "\n",
      "- **Position**: Senior Data Scientist\n",
      "- **Experience**: 5+ years\n",
      "- **Skills**: Python, TensorFlow, PyTorch\n",
      "- **Salary**: $120k-$180k\n",
      "- **Location**: San Francisco, CA (Hybrid)\n",
      "- **Deadline**: December 31, 2024\n",
      "\n",
      "\n",
      "Few-Shot Prompt Strategy:\n",
      "----------------------------------------\n",
      "{\"position\": \"Senior Data Scientist\", \"experience\": \"5+ years\", \"skills\": [\"Python\", \"TensorFlow\", \"PyTorch\"], \"salary\": \"$120k-$180k\", \"location\": \"San Francisco, CA\", \"deadline\": \"December 31, 2024\"}\n",
      "\n",
      "\n",
      "Chain-of-Thought Prompt Strategy:\n",
      "----------------------------------------\n",
      "```json\n",
      "{\n",
      "  \"job_title\": \"Senior Data Scientist\",\n",
      "  \"experience_requirements\": \"5+ years of experience in ML and NLP\",\n",
      "  \"technical_skills\": [\n",
      "    \"Python\",\n",
      "    \"TensorFlow\",\n",
      "    \"PyTorch\"\n",
      "  ],\n",
      "  \"salary_information\": \"$120k-$180k\",\n",
      "  \"location\": \"San Francisco, CA\",\n",
      "  \"work_arrangement\": \"Hybrid\",\n",
      "  \"application_deadline\": \"December 31, 2024\"\n",
      "}\n",
      "```\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Bonus: Batch Processing Multiple Job Postings\n",
      "================================================================================\n",
      "\n",
      "Batch Extraction Results:\n",
      "```json\n",
      "[\n",
      "    {\n",
      "        \"job_number\": 1,\n",
      "        \"position\": \"Senior Data Scientist\",\n",
      "        \"experience\": \"5+ years\",\n",
      "        \"skills\": [\"Python\", \"TensorFlow\", \"PyTorch\"],\n",
      "        \"salary\": \"$120k-$180k\",\n",
      "        \"location\": \"San Francisco, CA (Hybrid)\",\n",
      "        \"deadline\": \"December 31, 2024\"\n",
      "    },\n",
      "    {\n",
      "        \"job_number\": 2,\n",
      "        \"position\": \"Junior Frontend Developer\",\n",
      "        \"experience\": \"1-2 years\",\n",
      "        \"skills\": [\"React\", \"JavaScript\"],\n",
      "        \"salary\": \"$50k-$70k\",\n",
      "        \"location\": \"Remote\",\n",
      "        \"deadline\": \"No deadline specified\"\n",
      "    },\n",
      "    {\n",
      "        \"job_number\": 3,\n",
      "        \"position\": \"Lead DevOps Engineer\",\n",
      "        \"experience\": \"7+ years\",\n",
      "        \"skills\": [\"AWS\", \"Docker\", \"Kubernetes\"],\n",
      "        \"salary\": \"$150k-$200k\",\n",
      "        \"location\": \"Austin, TX office\",\n",
      "        \"deadline\": \"Rolling applications\"\n",
      "    }\n",
      "]\n",
      "```\n",
      "\n",
      "Note: Could not parse as JSON\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Key Takeaways from Exercise 3:\n",
      "1. Clear, structured prompts yield better results\n",
      "2. Specifying output format (JSON) improves consistency\n",
      "3. Few-shot examples can guide the model\n",
      "4. Chain-of-thought prompting helps with complex extractions\n",
      "5. Validation steps improve accuracy\n",
      "6. Different strategies work better for different tasks\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "job_posting = \"\"\"\n",
    "We are looking for a Senior Data Scientist with 5+ years of experience in ML and NLP.\n",
    "Must have Python, TensorFlow, and PyTorch skills. Salary: $120k-$180k.\n",
    "Location: San Francisco, CA (Hybrid). Apply by December 31, 2024.\n",
    "\"\"\"\n",
    "\n",
    "# Create a prompt to extract: position, experience, skills, salary, location, deadline\n",
    "# Your code here\n",
    "# Solution 1: Using OpenAI with structured JSON output\n",
    "print(\"Solution 1: OpenAI with Structured JSON Extraction\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "def extract_job_info_openai(job_text):\n",
    "    \"\"\"Extract structured information from job posting using OpenAI.\"\"\"\n",
    "    client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are an expert at extracting structured information from job postings. Always respond with valid JSON.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"\"\"Extract the following information from this job posting:\n",
    "\n",
    "Job Posting:\n",
    "{job_text}\n",
    "\n",
    "Extract and return a JSON object with these fields:\n",
    "- position: job title\n",
    "- experience: years of experience required (as a string, e.g., \"5+ years\")\n",
    "- skills: list of required technical skills\n",
    "- salary: salary range (as a string)\n",
    "- location: work location and type (e.g., \"San Francisco, CA (Hybrid)\")\n",
    "- deadline: application deadline (as a string)\n",
    "\n",
    "If any field is not found, use null.\n",
    "\n",
    "Respond ONLY with the JSON object, no additional text.\"\"\"}\n",
    "        ],\n",
    "        temperature=0\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Only run if API key is available\n",
    "if os.getenv(\"OPENAI_API_KEY\"):\n",
    "    result = extract_job_info_openai(job_posting)\n",
    "    print(\"\\nExtracted Information:\")\n",
    "    print(result)\n",
    "    \n",
    "    # Parse and pretty print\n",
    "    try:\n",
    "        parsed = json.loads(result)\n",
    "        print(\"\\nParsed JSON:\")\n",
    "        print(json.dumps(parsed, indent=2))\n",
    "    except json.JSONDecodeError:\n",
    "        print(\"\\nNote: Response may need cleaning to be valid JSON\")\n",
    "else:\n",
    "    print(\"OpenAI API key not found. Skipping OpenAI extraction.\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "\n",
    "# Solution 2: Using Claude with detailed prompt\n",
    "print(\"\\nSolution 2: Anthropic Claude with Detailed Extraction\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "def extract_job_info_claude(job_text):\n",
    "    \"\"\"Extract structured information using Claude.\"\"\"\n",
    "    client_anthropic = anthropic.Anthropic(api_key=os.getenv(\"ANTHROPIC_API_KEY\"))\n",
    "    \n",
    "    message = client_anthropic.messages.create(\n",
    "        model=\"claude-sonnet-4-5-20250929\",\n",
    "        max_tokens=1024,\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": f\"\"\"Analyze this job posting and extract structured information:\n",
    "\n",
    "Job Posting:\n",
    "{job_text}\n",
    "\n",
    "Please extract the following information and format as JSON:\n",
    "{{\n",
    "  \"position\": \"job title\",\n",
    "  \"experience\": \"years required\",\n",
    "  \"skills\": [\"skill1\", \"skill2\", ...],\n",
    "  \"salary\": \"salary range\",\n",
    "  \"location\": \"location and work type\",\n",
    "  \"deadline\": \"application deadline\"\n",
    "}}\n",
    "\n",
    "Be precise and extract only what's explicitly stated. Use null for missing fields.\"\"\"}\n",
    "        ]\n",
    "    )\n",
    "    return message.content[0].text\n",
    "\n",
    "# Only run if API key is available\n",
    "if os.getenv(\"ANTHROPIC_API_KEY\"):\n",
    "    result = extract_job_info_claude(job_posting)\n",
    "    print(\"\\nExtracted Information:\")\n",
    "    print(result)\n",
    "else:\n",
    "    print(\"Anthropic API key not found. Skipping Claude extraction.\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "\n",
    "# Solution 3: Multi-step extraction with validation\n",
    "print(\"\\nSolution 3: Multi-Step Extraction with Validation\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "def extract_and_validate_job_info(job_text):\n",
    "    \"\"\"Extract job info with validation and confidence scores.\"\"\"\n",
    "    client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "    \n",
    "    # Step 1: Extract information\n",
    "    extraction_response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are an expert at extracting structured information from job postings.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"\"\"Extract information from this job posting:\n",
    "\n",
    "{job_text}\n",
    "\n",
    "Provide a JSON object with:\n",
    "- position\n",
    "- experience\n",
    "- skills (as array)\n",
    "- salary\n",
    "- location\n",
    "- deadline\n",
    "\n",
    "Also include a \"confidence\" field (0-1) for each extracted piece of information.\"\"\"}\n",
    "        ],\n",
    "        temperature=0\n",
    "    )\n",
    "    \n",
    "    extraction = extraction_response.choices[0].message.content\n",
    "    \n",
    "    # Step 2: Validate and format\n",
    "    validation_response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You validate and format extracted job information.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"\"\"Review this extracted job information and ensure it's accurate:\n",
    "\n",
    "Original Job Posting:\n",
    "{job_text}\n",
    "\n",
    "Extracted Information:\n",
    "{extraction}\n",
    "\n",
    "Validate each field and provide:\n",
    "1. Corrected JSON if needed\n",
    "2. Brief notes on any corrections made\n",
    "3. Overall extraction quality (high/medium/low)\"\"\"}\n",
    "        ],\n",
    "        temperature=0\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        \"extraction\": extraction,\n",
    "        \"validation\": validation_response.choices[0].message.content\n",
    "    }\n",
    "\n",
    "# Only run if API key is available\n",
    "if os.getenv(\"OPENAI_API_KEY\"):\n",
    "    result = extract_and_validate_job_info(job_posting)\n",
    "    print(\"\\nStep 1 - Extraction:\")\n",
    "    print(result[\"extraction\"])\n",
    "    print(\"\\nStep 2 - Validation:\")\n",
    "    print(result[\"validation\"])\n",
    "else:\n",
    "    print(\"OpenAI API key not found. Skipping multi-step extraction.\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "\n",
    "# Solution 4: Comparison of different prompt strategies\n",
    "print(\"\\nSolution 4: Comparing Different Prompt Strategies\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "def compare_extraction_prompts(job_text):\n",
    "    \"\"\"Compare different prompting strategies.\"\"\"\n",
    "    client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "    \n",
    "    prompts = {\n",
    "        \"Basic\": f\"Extract job information from: {job_text}\",\n",
    "        \n",
    "        \"Structured\": f\"\"\"Extract these fields from the job posting:\n",
    "- Position\n",
    "- Experience\n",
    "- Skills\n",
    "- Salary\n",
    "- Location\n",
    "- Deadline\n",
    "\n",
    "Job Posting: {job_text}\"\"\",\n",
    "        \n",
    "        \"Few-Shot\": f\"\"\"Here's an example of job information extraction:\n",
    "\n",
    "Input: \"Hiring Junior Developer with 2 years experience. Python required. $60k-$80k. NYC.\"\n",
    "Output: {{\"position\": \"Junior Developer\", \"experience\": \"2 years\", \"skills\": [\"Python\"], \"salary\": \"$60k-$80k\", \"location\": \"NYC\", \"deadline\": null}}\n",
    "\n",
    "Now extract from this job posting:\n",
    "{job_text}\"\"\",\n",
    "        \n",
    "        \"Chain-of-Thought\": f\"\"\"Let's extract job information step by step:\n",
    "\n",
    "Job Posting: {job_text}\n",
    "\n",
    "Step 1: Identify the job title\n",
    "Step 2: Find experience requirements\n",
    "Step 3: List all technical skills mentioned\n",
    "Step 4: Extract salary information\n",
    "Step 5: Determine location and work arrangement\n",
    "Step 6: Find application deadline\n",
    "\n",
    "Provide the final answer as JSON.\"\"\"\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    for strategy, prompt in prompts.items():\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0\n",
    "        )\n",
    "        results[strategy] = response.choices[0].message.content\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Only run if API key is available\n",
    "if os.getenv(\"OPENAI_API_KEY\"):\n",
    "    comparison = compare_extraction_prompts(job_posting)\n",
    "    \n",
    "    for strategy, result in comparison.items():\n",
    "        print(f\"\\n{strategy} Prompt Strategy:\")\n",
    "        print(\"-\" * 40)\n",
    "        print(result)\n",
    "        print()\n",
    "else:\n",
    "    print(\"OpenAI API key not found. Skipping prompt comparison.\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "\n",
    "# Bonus: Extract from multiple job postings\n",
    "print(\"\\nBonus: Batch Processing Multiple Job Postings\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "job_postings = [\n",
    "    \"\"\"\n",
    "    We are looking for a Senior Data Scientist with 5+ years of experience in ML and NLP.\n",
    "    Must have Python, TensorFlow, and PyTorch skills. Salary: $120k-$180k.\n",
    "    Location: San Francisco, CA (Hybrid). Apply by December 31, 2024.\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    Junior Frontend Developer needed! 1-2 years experience with React and JavaScript.\n",
    "    $50k-$70k annually. Remote position. No deadline specified.\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    Lead DevOps Engineer - 7+ years required. AWS, Docker, Kubernetes essential.\n",
    "    Competitive salary $150k-$200k. Austin, TX office. Rolling applications.\n",
    "    \"\"\"\n",
    "]\n",
    "\n",
    "def batch_extract_jobs(job_list):\n",
    "    \"\"\"Extract information from multiple job postings.\"\"\"\n",
    "    client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "    \n",
    "    all_jobs = \"\\n\\n---\\n\\n\".join([f\"Job {i+1}:\\n{job}\" for i, job in enumerate(job_list)])\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You extract structured information from multiple job postings.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"\"\"Extract information from these job postings:\n",
    "\n",
    "{all_jobs}\n",
    "\n",
    "Return a JSON array where each element contains:\n",
    "- job_number\n",
    "- position\n",
    "- experience\n",
    "- skills\n",
    "- salary\n",
    "- location\n",
    "- deadline\"\"\"}\n",
    "        ],\n",
    "        temperature=0\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Only run if API key is available\n",
    "if os.getenv(\"OPENAI_API_KEY\"):\n",
    "    batch_results = batch_extract_jobs(job_postings)\n",
    "    print(\"\\nBatch Extraction Results:\")\n",
    "    print(batch_results)\n",
    "    \n",
    "    # Try to parse and format nicely\n",
    "    try:\n",
    "        parsed_batch = json.loads(batch_results)\n",
    "        print(\"\\n\\nFormatted Results:\")\n",
    "        for job in parsed_batch:\n",
    "            print(f\"\\n{job.get('position', 'Unknown Position')}:\")\n",
    "            for key, value in job.items():\n",
    "                if key != 'position':\n",
    "                    print(f\"  {key}: {value}\")\n",
    "    except json.JSONDecodeError:\n",
    "        print(\"\\nNote: Could not parse as JSON\")\n",
    "else:\n",
    "    print(\"OpenAI API key not found. Skipping batch extraction.\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"\\nKey Takeaways from Exercise 3:\")\n",
    "print(\"1. Clear, structured prompts yield better results\")\n",
    "print(\"2. Specifying output format (JSON) improves consistency\")\n",
    "print(\"3. Few-shot examples can guide the model\")\n",
    "print(\"4. Chain-of-thought prompting helps with complex extractions\")\n",
    "print(\"5. Validation steps improve accuracy\")\n",
    "print(\"6. Different strategies work better for different tasks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, you learned:\n",
    "1. Common NLP tasks and their applications\n",
    "2. Traditional text preprocessing techniques\n",
    "3. Modern tokenization with transformer models\n",
    "4. Zero-shot inference using OpenAI, Anthropic, and Hugging Face\n",
    "5. Prompt engineering basics for better results\n",
    "\n",
    "**Next Steps**: Practice with Quiz 1 and prepare for the live session on zero-shot inference!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iastate",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
